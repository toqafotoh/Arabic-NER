{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# â­ Arabic Named Entity Recognition (NER) Project\n\n### Project Overview\nIn this project, we'll be developing a Named Entity Recognition (NER) system for Arabic text to identify entities such as:\n- **Persons (PER)**\n- **Organizations (ORG)**\n- **Locations (LOC)**\n- **Dates (TIMEX)**\n---\n\n### Tools and Libraries:\n- HuggingFace Transformers\n- PyTorch\n- Kaggle Datasets\n- Tokenizers\n- Scikit-Learn","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:52.511448Z","iopub.execute_input":"2025-05-12T18:52:52.511776Z","iopub.status.idle":"2025-05-12T18:52:52.515836Z","shell.execute_reply.started":"2025-05-12T18:52:52.511753Z","shell.execute_reply":"2025-05-12T18:52:52.514930Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:52.517993Z","iopub.execute_input":"2025-05-12T18:52:52.518186Z","iopub.status.idle":"2025-05-12T18:52:55.690992Z","shell.execute_reply.started":"2025-05-12T18:52:52.518160Z","shell.execute_reply":"2025-05-12T18:52:55.690223Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\ndataset = load_dataset(\"iahlt/arabic_ner_mafat\")\ntrain_dataset = dataset[\"train\"]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:55.692807Z","iopub.execute_input":"2025-05-12T18:52:55.693060Z","iopub.status.idle":"2025-05-12T18:52:56.704766Z","shell.execute_reply.started":"2025-05-12T18:52:55.693038Z","shell.execute_reply":"2025-05-12T18:52:56.704215Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:56.705392Z","iopub.execute_input":"2025-05-12T18:52:56.705573Z","iopub.status.idle":"2025-05-12T18:52:56.710428Z","shell.execute_reply.started":"2025-05-12T18:52:56.705560Z","shell.execute_reply":"2025-05-12T18:52:56.709676Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'raw_tags', 'ner_tags', 'spaces', 'spans', 'record', 'text'],\n        num_rows: 40000\n    })\n})"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# Extract all the tags\nall_tags = set(tag for example in dataset[\"train\"]['raw_tags'] for tag in example)\n\n# Create a mapping from tag name to number\ntag2id = {tag: idx for idx, tag in enumerate(sorted(all_tags))}\nid2tag = {v: k for k, v in tag2id.items()}\n\n# Add new columns with numbers instead of raw_tags\ndef convert_tags(example):\n    example['labels'] = [tag2id[tag] for tag in example['raw_tags']]\n    return example\n\n# Apply the conversion to the entire dataset\ndataset = dataset.map(convert_tags)\n\n# If you want to see the mapping\nprint(\"Tag2ID:\", tag2id)\nprint(\"ID2Tag:\", id2tag)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:56.712107Z","iopub.execute_input":"2025-05-12T18:52:56.712330Z","iopub.status.idle":"2025-05-12T18:52:58.366817Z","shell.execute_reply.started":"2025-05-12T18:52:56.712314Z","shell.execute_reply":"2025-05-12T18:52:58.366216Z"}},"outputs":[{"name":"stdout","text":"Tag2ID: {'B-ANG': 0, 'B-DUC': 1, 'B-EVE': 2, 'B-FAC': 3, 'B-GPE': 4, 'B-INFORMAL': 5, 'B-LOC': 6, 'B-MISC': 7, 'B-ORG': 8, 'B-PER': 9, 'B-TIMEX': 10, 'B-TTL': 11, 'B-WOA': 12, 'I-ANG': 13, 'I-DUC': 14, 'I-EVE': 15, 'I-FAC': 16, 'I-GPE': 17, 'I-INFORMAL': 18, 'I-LOC': 19, 'I-MISC': 20, 'I-ORG': 21, 'I-PER': 22, 'I-TIMEX': 23, 'I-TTL': 24, 'I-WOA': 25, 'L-ANG': 26, 'L-DUC': 27, 'L-EVE': 28, 'L-FAC': 29, 'L-GPE': 30, 'L-INFORMAL': 31, 'L-LOC': 32, 'L-MISC': 33, 'L-ORG': 34, 'L-PER': 35, 'L-TIMEX': 36, 'L-TTL': 37, 'L-WOA': 38, 'O': 39}\nID2Tag: {0: 'B-ANG', 1: 'B-DUC', 2: 'B-EVE', 3: 'B-FAC', 4: 'B-GPE', 5: 'B-INFORMAL', 6: 'B-LOC', 7: 'B-MISC', 8: 'B-ORG', 9: 'B-PER', 10: 'B-TIMEX', 11: 'B-TTL', 12: 'B-WOA', 13: 'I-ANG', 14: 'I-DUC', 15: 'I-EVE', 16: 'I-FAC', 17: 'I-GPE', 18: 'I-INFORMAL', 19: 'I-LOC', 20: 'I-MISC', 21: 'I-ORG', 22: 'I-PER', 23: 'I-TIMEX', 24: 'I-TTL', 25: 'I-WOA', 26: 'L-ANG', 27: 'L-DUC', 28: 'L-EVE', 29: 'L-FAC', 30: 'L-GPE', 31: 'L-INFORMAL', 32: 'L-LOC', 33: 'L-MISC', 34: 'L-ORG', 35: 'L-PER', 36: 'L-TIMEX', 37: 'L-TTL', 38: 'L-WOA', 39: 'O'}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# ğŸ·ï¸ Tag2ID Mappings for Arabic NER Dataset\n\nThe table below explains each NER tag, its meaning, along with multiple Arabic examples to cover various cases:\n\n<table style=\"border-collapse: collapse; width: 100%; text-align: center;\">\n  <thead style=\"background-color: #f2f2f2; color: #333;\">\n    <tr>\n      <th style=\"padding: 10px; border: 1px solid #ccc;\">Tag</th>\n      <th style=\"padding: 10px; border: 1px solid #ccc;\">Meaning</th>\n      <th style=\"padding: 10px; border: 1px solid #ccc;\">Arabic Examples</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">PER</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Person ğŸ‘¤</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"Ø£Ø­Ù…Ø¯\"ØŒ \"ÙØ§Ø·Ù…Ø©\"ØŒ \"Ù…Ø­Ù…Ø¯ ØµÙ„Ø§Ø­\"ØŒ \"Ù†Ø¬ÙŠØ¨ Ù…Ø­ÙÙˆØ¸\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">ORG</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Organization ğŸ¢</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"Ø´Ø±ÙƒØ© Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª\"ØŒ \"Ø§Ù„ÙŠÙˆÙ†ÙŠØ³ÙƒÙˆ\"ØŒ \"Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©\"ØŒ \"Ù†Ø§Ø¯ÙŠ Ø§Ù„Ø£Ù‡Ù„ÙŠ\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">LOC</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Location ğŸ“</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"Ø§Ù„Ø­Ø¯ÙŠÙ‚Ø©\"ØŒ \"Ø§Ù„Ù†Ù‡Ø±\"ØŒ \"Ø§Ù„ØµØ­Ø±Ø§Ø¡\"ØŒ \"Ø§Ù„Ø¬Ø¨Ù„\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">GPE</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Geopolitical Entity ğŸŒ</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"Ù…ØµØ±\"ØŒ \"Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©\"ØŒ \"Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©\"ØŒ \"Ø§Ù„Ø±ÙŠØ§Ø¶\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">TIMEX</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Time Expression â°</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"Ø§Ù„Ø³Ø§Ø¹Ø©\"ØŒ \"Ø§Ù„ÙŠÙˆÙ…\"ØŒ \"Ø¹Ø§Ù… Ù¢Ù Ù¢Ù \"ØŒ \"Ù…Ù†ØªØµÙ Ø§Ù„Ù„ÙŠÙ„\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">TTL</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Title ğŸ“š</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙƒØªØ§Ø¨\"ØŒ \"Ø±ÙˆØ§ÙŠØ© Ø§Ù„Ø£Ø³ÙˆØ¯ ÙŠÙ„ÙŠÙ‚ Ø¨Ùƒ\"ØŒ \"ÙÙŠÙ„Ù… Ø§Ù„ÙƒÙ†Ø²\"ØŒ \"Ù…Ù‚Ø§Ù„Ø© Ø¹Ù„Ù…ÙŠØ©\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">WOA</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Work of Art ğŸ¨</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙˆÙ†Ø§Ù„ÙŠØ²Ø§\"ØŒ \"ØªÙ…Ø«Ø§Ù„ Ø§Ù„Ø­Ø±ÙŠØ©\"ØŒ \"Ù‚ØµÙŠØ¯Ø© Ø§Ù„Ø£Ø·Ù„Ø§Ù„\"ØŒ \"Ù…Ù‚Ø·ÙˆØ¹Ø© Ù…ÙˆØ³ÙŠÙ‚ÙŠØ©\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">MISC</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Miscellaneous ğŸ“¦</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"Ù…ÙˆÙ‚Ø¹ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ\"ØŒ \"Ø§Ø³Ù… Ø­Ø¯Ø«\"ØŒ \"Ù…ØµØ·Ù„Ø­ Ø¹Ù„Ù…ÙŠ\"ØŒ \"Ø¨Ø±Ø§Ø¡Ø© Ø§Ø®ØªØ±Ø§Ø¹\"</td>\n    </tr>\n  </tbody>\n</table>\n\n> This table gives a richer and clearer view ğŸŒŸ of each tag with several examples to better understand how tags are used in Arabic contexts.\n","metadata":{}},{"cell_type":"code","source":"dataset[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.367479Z","iopub.execute_input":"2025-05-12T18:52:58.367681Z","iopub.status.idle":"2025-05-12T18:52:58.373290Z","shell.execute_reply.started":"2025-05-12T18:52:58.367666Z","shell.execute_reply":"2025-05-12T18:52:58.372594Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'tokens': ['Ø§Ù„Ø¨Ø±ÙˆÙØ³ÙˆØ±', 'Ù…Ø­Ù…ÙˆØ¯', 'Ø®Ù„ÙŠÙ„'],\n 'raw_tags': ['B-TTL', 'B-PER', 'L-PER'],\n 'ner_tags': [47, 38, 37],\n 'spaces': [1, 1, 0],\n 'spans': [{'end': 9, 'label': 'TTL', 'start': 0, 'text': 'Ø§Ù„Ø¨Ø±ÙˆÙØ³ÙˆØ±'},\n  {'end': 20, 'label': 'PER', 'start': 10, 'text': 'Ù…Ø­Ù…ÙˆØ¯ Ø®Ù„ÙŠÙ„'}],\n 'record': '{\"metadata\": {\"doc_id\": \"003875358633647be47857dcee7869cfc03720431774756bd997a7cf87dd93bb\", \"url\": \"https://www.alarab.com//Article/824736\", \"source\": \"AlArab\", \"title\": \"ÙƒÙ„ÙŠØ© Ø³Ø®Ù†ÙŠÙ† Ù„ØªØ§Ù”Ù‡ÙŠÙ„ Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ø¹ØªØ±Ø§Ù Ù…Ø¬Ù„Ø³ Ø§Ù„ØªØ¹Ù„ÙŠÙ… Ø§Ù„Ø¹Ø§Ù„ÙŠ Ø¨Ø§Ù•Ø¹Ø·Ø§Ø¡ Ù„Ù‚Ø¨ M.Teach\", \"authors\": \"Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ø±Ø¨ ÙˆØµØ­ÙŠÙØ© ÙƒÙ„ Ø§Ù„Ø¹Ø±Ø¨- Ø§Ù„Ù†Ø§ØµØ±Ø©\", \"date\": \"2017-09-14 15:59:34\", \"domains\": \"Students\", \"parnumber\": \"8\", \"sentnumber\": \"none\"}, \"text\": \"Ø§Ù„Ø¨Ø±ÙˆÙØ³ÙˆØ± Ù…Ø­Ù…ÙˆØ¯ Ø®Ù„ÙŠÙ„\", \"label\": [[0, 9, \"TTL\"], [10, 20, \"PER\"]], \"user\": \"nlhowell\", \"timestamp\": 1685356355.6331542, \"flatten\": {\"tokens\": [\"Ø§Ù„Ø¨Ø±ÙˆÙØ³ÙˆØ±\", \"Ù…Ø­Ù…ÙˆØ¯\", \"Ø®Ù„ÙŠÙ„\"], \"ner_tags\": [\"B-TTL\", \"B-PER\", \"L-PER\"], \"spaces\": [1, 1, 0]}, \"label_hierarchy\": {\"0\": [{\"end\": 9, \"label\": \"TTL\", \"start\": 0, \"text\": \"Ø§Ù„Ø¨Ø±ÙˆÙØ³ÙˆØ±\"}, {\"end\": 20, \"label\": \"PER\", \"start\": 10, \"text\": \"Ù…Ø­Ù…ÙˆØ¯ Ø®Ù„ÙŠÙ„\"}], \"1\": null, \"2\": null, \"3\": null, \"4\": null, \"5\": null}, \"has_overlappings\": false, \"n_hierarchy_levels\": 1}',\n 'text': 'Ø§Ù„Ø¨Ø±ÙˆÙØ³ÙˆØ± Ù…Ø­Ù…ÙˆØ¯ Ø®Ù„ÙŠÙ„',\n 'labels': [11, 9, 35]}"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from datasets import DatasetDict\n\ndef simplify(example):\n    return {\n        \"tokens\": example[\"tokens\"],\n        \"labels\": example[\"labels\"]\n    }\n\nkeep_cols = [\"tokens\", \"labels\"]\n\nsimplified_dataset = DatasetDict({\n    split: dataset[split]\n        .map(simplify, remove_columns=[col for col in dataset[split].column_names if col not in keep_cols])\n    for split in dataset\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.373987Z","iopub.execute_input":"2025-05-12T18:52:58.374206Z","iopub.status.idle":"2025-05-12T18:52:58.392209Z","shell.execute_reply.started":"2025-05-12T18:52:58.374182Z","shell.execute_reply":"2025-05-12T18:52:58.391629Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"simplified_dataset[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.392833Z","iopub.execute_input":"2025-05-12T18:52:58.393091Z","iopub.status.idle":"2025-05-12T18:52:58.406321Z","shell.execute_reply.started":"2025-05-12T18:52:58.393068Z","shell.execute_reply":"2025-05-12T18:52:58.405743Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'tokens': ['Ø§Ù„Ø¨Ø±ÙˆÙØ³ÙˆØ±', 'Ù…Ø­Ù…ÙˆØ¯', 'Ø®Ù„ÙŠÙ„'], 'labels': [11, 9, 35]}"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"# ğŸ¥° Dataset Preprocessing Summary\n\nIn this project, we focused on preprocessing the **Arabic NER** dataset by following these steps:\n\n### 1. **Selecting Important Columns**  \nWe selected only the necessary columns: **tokens** and **labels**. This helps to focus on the key information needed for Named Entity Recognition (NER).\n\n### 2. **Tag Mapping**  \nWe created a mapping between the tag names and numerical values:\n- **Tag to ID Mapping**: We assigned a unique number to each tag.\n- **ID to Tag Mapping**: We reversed the mapping to get back from IDs to tags.\n\n### 3. **Unified Tagging**  \nWe replaced the original **raw_tags** with numerical labels corresponding to each entity type, making it easier to process the data for training.\n\n---\n\nWith these steps, the dataset is now ready for model training and optimized for NER tasks! ğŸš€\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n\nnum_labels = 40\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"asafaya/bert-base-arabic\",\n    num_labels=num_labels,\n    id2label=id2tag,\n    label2id=tag2id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.406848Z","iopub.execute_input":"2025-05-12T18:52:58.407002Z","iopub.status.idle":"2025-05-12T18:52:58.727258Z","shell.execute_reply.started":"2025-05-12T18:52:58.406989Z","shell.execute_reply":"2025-05-12T18:52:58.726645Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"\n# âœ¨ Tokenization and Label Alignment for Arabic NER\n\nTo prepare the Arabic NER dataset for model training, we apply a tokenization and label alignment step.  \nThe goal is to **tokenize** each word properly and **align** the NER labels with the corresponding tokens, taking into account that some words may be split into multiple sub-tokens.\n\n---\n\n### ğŸ”¹ Step Explanation:\n\n- 1ï¸âƒ£ **Tokenization:** Tokenize each example separately using the pretrained tokenizer.\n- 2ï¸âƒ£ **Tracking:** Keep track of which token belongs to which original word (`word_ids`).\n- 3ï¸âƒ£ **Label Assignment:** Assign the correct label only to the first token of each word. For sub-tokens or special tokens, assign `-100` (to ignore them during loss calculation).\n- 4ï¸âƒ£ **Store Labels:** Attach the aligned labels back into the tokenized output.\n- 5ï¸âƒ£ **Batch Processing:** Apply the mapping to the entire dataset using `batched=True` for efficiency.\n\n---\n\n### ğŸ¯ Why This Matters?\n\n- **Subword Tokenization:** In languages like Arabic, tokenizers (such as BERT-based ones) often split words into multiple subwords.\n- **Correct Label Alignment:** Ensures only the first sub-token carries the label, avoiding confusion during model training.\n- **Loss Masking:** Using `-100` masks irrelevant positions, helping the model focus on learning the right targets.\n","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        padding=True,\n        max_length=512\n    )\n\n    all_labels = []\n    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n        labels = []\n        previous_word_idx = None\n        for word_idx in word_ids:\n            if word_idx is None:\n                labels.append(-100)\n            elif word_idx != previous_word_idx:\n                labels.append(examples[\"labels\"][i][word_idx])\n            else:\n                labels.append(-100)\n            previous_word_idx = word_idx\n        all_labels.append(labels)\n\n    tokenized_inputs[\"labels\"] = all_labels\n    return tokenized_inputs\n\n\ntokenized_datasets = simplified_dataset.map(tokenize_and_align_labels, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.727861Z","iopub.execute_input":"2025-05-12T18:52:58.728050Z","iopub.status.idle":"2025-05-12T18:52:58.745608Z","shell.execute_reply.started":"2025-05-12T18:52:58.728036Z","shell.execute_reply":"2025-05-12T18:52:58.744875Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.747976Z","iopub.execute_input":"2025-05-12T18:52:58.748320Z","iopub.status.idle":"2025-05-12T18:53:01.856567Z","shell.execute_reply.started":"2025-05-12T18:52:58.748303Z","shell.execute_reply":"2025-05-12T18:53:01.855611Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!pip install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:01.857577Z","iopub.execute_input":"2025-05-12T18:53:01.857812Z","iopub.status.idle":"2025-05-12T18:53:04.758363Z","shell.execute_reply.started":"2025-05-12T18:53:01.857791Z","shell.execute_reply":"2025-05-12T18:53:04.757594Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"\n# ğŸ“Š Evaluation Setup for Arabic NER\n\nTo properly evaluate our NER model performance, we set up the data collator, evaluation metric, and metric computation function.\n\n---\n\n### ğŸ”¹ Key Steps:\n\n- **Data Collation:**  \n  Use `DataCollatorForTokenClassification` to dynamically pad inputs and labels during training.\n\n- **Metric Loading:**  \n  Load the `seqeval` metric, specialized for sequence labeling tasks like NER.\n\n- **Metric Computation:**  \n  - Predict the best label for each token using `argmax`.\n  - Align predictions and labels, ignoring padding tokens (`-100`).\n  - Calculate overall **precision**, **recall**, **f1-score**, and **accuracy**.\n\n \n","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\nimport numpy as np\nimport evaluate\n\n# Data collator\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\n# Metrics\nmetric = evaluate.load(\"seqeval\")\n\ndef compute_metrics(p):\n    predictions, labels = p\n    # Ensure that predictions is an array of probabilities, and if so, use np.argmax to choose the label with the highest probability\n    predictions = np.argmax(predictions, axis=2)\n\n    # Get the true predictions (selected based on the highest probability)\n    true_predictions = [\n        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    # Get the true labels (from labels, removing the -100 values)\n    true_labels = [\n        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    # Calculate the results using the seqeval library\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n\n    # Return the evaluation metrics\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:04.759375Z","iopub.execute_input":"2025-05-12T18:53:04.759604Z","iopub.status.idle":"2025-05-12T18:53:05.263307Z","shell.execute_reply.started":"2025-05-12T18:53:04.759569Z","shell.execute_reply":"2025-05-12T18:53:05.262713Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torch\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:05.263993Z","iopub.execute_input":"2025-05-12T18:53:05.264196Z","iopub.status.idle":"2025-05-12T18:53:05.459669Z","shell.execute_reply.started":"2025-05-12T18:53:05.264158Z","shell.execute_reply":"2025-05-12T18:53:05.458942Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=40, bias=True)\n)"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"import os\nimport warnings\nimport logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:05.460509Z","iopub.execute_input":"2025-05-12T18:53:05.460786Z","iopub.status.idle":"2025-05-12T18:53:05.466269Z","shell.execute_reply.started":"2025-05-12T18:53:05.460763Z","shell.execute_reply":"2025-05-12T18:53:05.465561Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"\n# ğŸš€ Model Training Setup for Arabic NER\n\nIn this section, we use the **Trainer API** from Hugging Face Transformers to train the model **CAMeL-Lab's bert-base-arabic-camelbert-ca** (`camelbert`) on the **Arabic NER dataset**.\n\n\n\n### ğŸ”¹ Training Arguments Explanation:\n\n- **output_dir=\"/kaggle/working/\"**: Specifies the directory where model checkpoints and training results will be saved.\n- **eval_strategy=\"epoch\"**: Evaluates the model at the end of each *epoch*.\n- **save_strategy=\"epoch\"**: Saves the model after each *epoch*.\n- **learning_rate=2e-5**: Sets the learning rate for the optimizer.\n- **per_device_train_batch_size=16**: Batch size per device (CPU/GPU) during training.\n- **per_device_eval_batch_size=16**: Batch size per device during evaluation.\n- **num_train_epochs=3**: Number of *epochs* for training, set to 3 here.\n- **weight_decay=0.01**: Applies L2 regularization to prevent overfitting.\n- **disable_tqdm=False**: Enables the progress bar during training.\n- **gradient_checkpointing=True**: Saves memory by recomputing activations during the backward pass (useful for large models).\n- **dataloader_num_workers=4**: Number of workers for data loading to speed up the process.\n\n\n\n### ğŸ”¹ Trainer Setup Explanation:\n\n- **model=model.to(device)**: Moves the model to the appropriate device (CPU or GPU).\n- **args=training_args**: Uses the specified training arguments defined earlier.\n- **train_dataset=tokenized_datasets[\"train\"]**: The dataset used for training after preprocessing.\n- **eval_dataset=tokenized_datasets[\"train\"].select(range(1000))**: A subset of 1000 examples for evaluation.\n- **tokenizer=tokenizer**: The tokenizer used to convert text into tokens.\n- **data_collator=data_collator**: Responsible for padding the inputs to the correct size during batching.\n- **compute_metrics=compute_metrics**: A function that calculates Precision, Recall, F1, and overall Accuracy.\n\n\n\n### ğŸ› ï¸ Final Step: Training\n\nFinally, we train the model using `trainer.train()`, which starts the training process on the **Arabic NER dataset**.\n\n\n","metadata":{}},{"cell_type":"code","source":"tokenized_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\nval_test_split = tokenized_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n\ntrain_dataset = tokenized_split[\"train\"]\nvalid_dataset = val_test_split[\"train\"]\ntest_dataset = val_test_split[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:05.466963Z","iopub.execute_input":"2025-05-12T18:53:05.467266Z","iopub.status.idle":"2025-05-12T18:53:05.491226Z","shell.execute_reply.started":"2025-05-12T18:53:05.467211Z","shell.execute_reply":"2025-05-12T18:53:05.490576Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom transformers import Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    disable_tqdm=False,\n    gradient_checkpointing=True\n)\n\ntrainer = Trainer(\n    model=model.to(device),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:05.492329Z","iopub.execute_input":"2025-05-12T18:53:05.492591Z","iopub.status.idle":"2025-05-12T21:03:14.069088Z","shell.execute_reply.started":"2025-05-12T18:53:05.492568Z","shell.execute_reply":"2025-05-12T21:03:14.068357Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6000/6000 2:10:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.190300</td>\n      <td>0.179529</td>\n      <td>0.746202</td>\n      <td>0.800179</td>\n      <td>0.772249</td>\n      <td>0.948414</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.139800</td>\n      <td>0.166288</td>\n      <td>0.761528</td>\n      <td>0.813257</td>\n      <td>0.786543</td>\n      <td>0.951965</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.106800</td>\n      <td>0.173279</td>\n      <td>0.767163</td>\n      <td>0.820879</td>\n      <td>0.793112</td>\n      <td>0.953025</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6000, training_loss=0.17119370396931965, metrics={'train_runtime': 7807.3911, 'train_samples_per_second': 12.296, 'train_steps_per_second': 0.769, 'total_flos': 2.495136155267712e+16, 'train_loss': 0.17119370396931965, 'epoch': 3.0})"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"results = trainer.evaluate(eval_dataset=test_dataset)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:03:14.069938Z","iopub.execute_input":"2025-05-12T21:03:14.070142Z","iopub.status.idle":"2025-05-12T21:04:21.341770Z","shell.execute_reply.started":"2025-05-12T21:03:14.070127Z","shell.execute_reply":"2025-05-12T21:04:21.340913Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 01:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.17174355685710907, 'eval_precision': 0.7637311620598529, 'eval_recall': 0.8175076452599388, 'eval_f1': 0.7897049591964846, 'eval_accuracy': 0.9523553162853298, 'eval_runtime': 67.229, 'eval_samples_per_second': 59.498, 'eval_steps_per_second': 3.719, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from torchinfo import summary\n\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Trainable parameters: {trainable_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:17:07.257032Z","iopub.execute_input":"2025-05-12T21:17:07.257643Z","iopub.status.idle":"2025-05-12T21:17:07.283276Z","shell.execute_reply.started":"2025-05-12T21:17:07.257617Z","shell.execute_reply":"2025-05-12T21:17:07.282646Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 110057512\nTrainable parameters: 110057512\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"\n\n# ğŸ“ Test Sentences for Arabic NER\n\nThis section includes a set of test sentences used to evaluate the performance of the Arabic NER model.\n","metadata":{}},{"cell_type":"code","source":"test_sentences_ner = [\n    \"Ø³Ø§ÙØ± Ø£Ø­Ù…Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© Ù„Ø­Ø¶ÙˆØ± Ù…Ø¤ØªÙ…Ø± Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§.\",\n    \"Ù…ÙÙ†Ø­ ÙƒØªØ§Ø¨ 'Ø§Ù„Ø®ÙŠÙ…ÙŠØ§Ø¦ÙŠ' Ø¬Ø§Ø¦Ø²Ø© Ø£ÙØ¶Ù„ Ø±ÙˆØ§ÙŠØ©.\",\n    \"Ø³ØªÙ‚Ø§Ù… Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙÙŠ Ù…Ù„Ø¹Ø¨ Ø§Ù„Ù…Ù„Ùƒ ÙÙ‡Ø¯ Ø§Ù„Ø¯ÙˆÙ„ÙŠ.\",\n    \"ØªØ¹Ù…Ù„ Ù„ÙŠÙ„Ù‰ ÙÙŠ Ø´Ø±ÙƒØ© Ù…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª Ù…Ù†Ø° Ø®Ù…Ø³ Ø³Ù†ÙˆØ§Øª.\",\n    \"ÙˆÙ„Ø¯ Ø§Ù„Ø¹Ø§Ù„Ù… Ø¥Ø³Ø­Ø§Ù‚ Ù†ÙŠÙˆØªÙ† ÙÙŠ Ø¥Ù†Ø¬Ù„ØªØ±Ø§.\",\n    \"Ø£Ù‚ÙŠÙ…Øª ÙØ¹Ø§Ù„ÙŠØ§Øª Ù…Ø¹Ø±Ø¶ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø¯ÙˆÙ„ÙŠ ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶.\",\n    \"ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø³Ø¨ØªÙ…Ø¨Ø±.\",\n    \"ÙŠØªØ­Ø¯Ø« Ø³Ø§Ù…ÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©.\",\n    \"Ø²Ø§Ø± Ø§Ù„Ø³Ø§Ø¦Ø­ÙˆÙ† Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¨ØªØ±Ø§Ø¡ Ø§Ù„Ø£Ø«Ø±ÙŠØ© ÙÙŠ Ø§Ù„Ø£Ø±Ø¯Ù†.\",\n    \"ÙŠÙ‚Ø¯Ù… Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„ÙˆØ·Ù†ÙŠ Ø®Ø¯Ù…Ø§Øª Ø·Ø¨ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©.\",\n    \"ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¹Ù„Ù‰ Ù‚Ù†Ø§Ø© Ø§Ù„Ø¬Ø²ÙŠØ±Ø©.\",\n    \"Ø§ÙØªØªØ­Øª Ø´Ø±ÙƒØ© Ø³Ø§Ù…Ø³ÙˆÙ†Ø¬ ÙØ±Ø¹Ø§Ù‹ Ø¬Ø¯ÙŠØ¯Ø§Ù‹ ÙÙŠ Ø¯Ø¨ÙŠ.\",\n    \"Ø³ÙŠØªÙ… ØªØ³Ù„ÙŠÙ… Ø§Ù„Ø¬ÙˆØ§Ø¦Ø² ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³ Ø§Ù„Ù‚Ø§Ø¯Ù….\",\n    \"Ø£Ø­Ø¨ Ù‚Ø±Ø§Ø¡Ø© ÙƒØªØ§Ø¨ 'Ù…Ø¦Ø© Ø¹Ø§Ù… Ù…Ù† Ø§Ù„Ø¹Ø²Ù„Ø©'.\",\n    \"ØªØ¹ÙŠÙ‘Ù† Ø§Ù„Ø¯ÙƒØªÙˆØ± ÙŠÙˆØ³Ù Ø¹Ù…ÙŠØ¯Ø§Ù‹ Ù„ÙƒÙ„ÙŠØ© Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©.\",\n    \"Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ø³Ø¨Ø§Ù†ÙŠØ© Ù…Ù†ØªØ´Ø±Ø© ÙÙŠ Ù‚Ø§Ø±Ø© Ø£Ù…Ø±ÙŠÙƒØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ©.\",\n    \"Ø²Ø§Ø± Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ø±Ø¦Ø§Ø³ÙŠ Ù‚ØµØ± Ù‚Ø±Ø·Ø§Ø¬ Ø§Ù„Ø±Ø¦Ø§Ø³ÙŠ ÙÙŠ ØªÙˆÙ†Ø³.\",\n    \"ÙŠØ¨Ø¯Ø£ Ù…Ù‡Ø±Ø¬Ø§Ù† ÙƒØ§Ù† Ø§Ù„Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ ÙÙŠ Ù…Ø§ÙŠÙˆ Ù…Ù† ÙƒÙ„ Ø¹Ø§Ù….\",\n    \"ØªØ¹Ø¯ Ø¬Ø¨Ø§Ù„ Ø§Ù„Ù‡ÙŠÙ…Ø§Ù„Ø§ÙŠØ§ Ù…Ù† Ø£Ø¹Ù„Ù‰ Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø¬Ø¨Ø§Ù„ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù….\",\n    \"ØªÙ… Ø¥Ø¹Ù„Ø§Ù† Ø§Ù„ÙØ§Ø¦Ø² Ø¨Ø¬Ø§Ø¦Ø²Ø© Ù†ÙˆØ¨Ù„ Ù„Ù„Ø³Ù„Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹.\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:04:21.342672Z","iopub.execute_input":"2025-05-12T21:04:21.343434Z","iopub.status.idle":"2025-05-12T21:04:21.347475Z","shell.execute_reply.started":"2025-05-12T21:04:21.343411Z","shell.execute_reply":"2025-05-12T21:04:21.346737Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"for sentence in test_sentences_ner:\n    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    logits_cpu = logits.cpu().numpy()\n    predictions = np.argmax(logits_cpu, axis=2)\n    predicted_tags = [id2tag[pred] for pred in predictions[0]]\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n\n    print(f\"Sentence: {sentence}\")\n    for token, tag in zip(tokens, predicted_tags):\n        print(f\"{token}: {tag}\")\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:04:21.348200Z","iopub.execute_input":"2025-05-12T21:04:21.348376Z","iopub.status.idle":"2025-05-12T21:04:21.526158Z","shell.execute_reply.started":"2025-05-12T21:04:21.348363Z","shell.execute_reply":"2025-05-12T21:04:21.525575Z"}},"outputs":[{"name":"stdout","text":"Sentence: Ø³Ø§ÙØ± Ø£Ø­Ù…Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© Ù„Ø­Ø¶ÙˆØ± Ù…Ø¤ØªÙ…Ø± Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§.\n[CLS]: O\nØ³Ø§ÙØ±: O\nØ§Ø­Ù…Ø¯: B-PER\nØ§Ù„Ù‰: O\nØ§Ù„Ù‚Ø§Ù‡Ø±Ø©: B-GPE\nÙ„Ø­Ø¶ÙˆØ±: O\nÙ…ÙˆØªÙ…Ø±: B-EVE\nØ§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§: L-EVE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: Ù…ÙÙ†Ø­ ÙƒØªØ§Ø¨ 'Ø§Ù„Ø®ÙŠÙ…ÙŠØ§Ø¦ÙŠ' Ø¬Ø§Ø¦Ø²Ø© Ø£ÙØ¶Ù„ Ø±ÙˆØ§ÙŠØ©.\n[CLS]: O\nÙ…Ù†Ø­: O\nÙƒØªØ§Ø¨: O\n': O\nØ§Ù„Ø®ÙŠ: B-WOA\n##Ù…ÙŠ: L-WOA\n##Ø§ÙŠÙŠ: L-WOA\n': O\nØ¬Ø§ÙŠØ²Ø©: O\nØ§ÙØ¶Ù„: O\nØ±ÙˆØ§ÙŠØ©: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: Ø³ØªÙ‚Ø§Ù… Ø§Ù„Ù…Ø¨Ø§Ø±Ø§Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙÙŠ Ù…Ù„Ø¹Ø¨ Ø§Ù„Ù…Ù„Ùƒ ÙÙ‡Ø¯ Ø§Ù„Ø¯ÙˆÙ„ÙŠ.\n[CLS]: O\nØ³ØªÙ‚Ø§Ù…: O\nØ§Ù„Ù…Ø¨Ø§Ø±Ø§Ø©: O\nØ§Ù„Ù†Ù‡Ø§ÙŠÙŠØ©: O\nÙÙŠ: O\nÙ…Ù„Ø¹Ø¨: B-FAC\nØ§Ù„Ù…Ù„Ùƒ: I-FAC\nÙÙ‡Ø¯: I-FAC\nØ§Ù„Ø¯ÙˆÙ„ÙŠ: L-FAC\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ØªØ¹Ù…Ù„ Ù„ÙŠÙ„Ù‰ ÙÙŠ Ø´Ø±ÙƒØ© Ù…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª Ù…Ù†Ø° Ø®Ù…Ø³ Ø³Ù†ÙˆØ§Øª.\n[CLS]: O\nØªØ¹Ù…Ù„: O\nÙ„ÙŠÙ„Ù‰: B-PER\nÙÙŠ: O\nØ´Ø±ÙƒØ©: O\nÙ…Ø§ÙŠÙƒØ±ÙˆØ³ÙˆÙØª: B-ORG\nÙ…Ù†Ø°: O\nØ®Ù…Ø³: O\nØ³Ù†ÙˆØ§Øª: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ÙˆÙ„Ø¯ Ø§Ù„Ø¹Ø§Ù„Ù… Ø¥Ø³Ø­Ø§Ù‚ Ù†ÙŠÙˆØªÙ† ÙÙŠ Ø¥Ù†Ø¬Ù„ØªØ±Ø§.\n[CLS]: O\nÙˆÙ„Ø¯: O\nØ§Ù„Ø¹Ø§Ù„Ù…: B-TTL\nØ§Ø³Ø­Ø§Ù‚: B-PER\nÙ†ÙŠÙˆØª: L-PER\n##Ù†: L-PER\nÙÙŠ: O\nØ§Ù†Ø¬Ù„ØªØ±Ø§: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: Ø£Ù‚ÙŠÙ…Øª ÙØ¹Ø§Ù„ÙŠØ§Øª Ù…Ø¹Ø±Ø¶ Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø¯ÙˆÙ„ÙŠ ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶.\n[CLS]: O\nØ§Ù‚ÙŠÙ…Øª: O\nÙØ¹Ø§Ù„ÙŠØ§Øª: O\nÙ…Ø¹Ø±Ø¶: B-EVE\nØ§Ù„ÙƒØªØ§Ø¨: I-EVE\nØ§Ù„Ø¯ÙˆÙ„ÙŠ: L-EVE\nÙÙŠ: O\nØ§Ù„Ø±ÙŠØ§Ø¶: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø¹Ø§Ù… Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø³Ø¨ØªÙ…Ø¨Ø±.\n[CLS]: O\nÙŠØ¨Ø¯Ø§: O\nØ§Ù„Ø¹Ø§Ù…: O\nØ§Ù„Ø¯Ø±Ø§Ø³ÙŠ: O\nØ§Ù„Ø¬Ø¯ÙŠØ¯: O\nÙÙŠ: O\nØ³Ø¨ØªÙ…Ø¨Ø±: B-TIMEX\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ÙŠØªØ­Ø¯Ø« Ø³Ø§Ù…ÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©.\n[CLS]: O\nÙŠØªØ­Ø¯Ø«: O\nØ³Ø§Ù…ÙŠ: B-PER\nØ§Ù„Ù„ØºØ©: B-ANG\nØ§Ù„ÙØ±Ù†Ø³ÙŠØ©: L-ANG\nØ¨Ø·Ù„: O\n##Ø§Ù‚Ø©: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: Ø²Ø§Ø± Ø§Ù„Ø³Ø§Ø¦Ø­ÙˆÙ† Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ø¨ØªØ±Ø§Ø¡ Ø§Ù„Ø£Ø«Ø±ÙŠØ© ÙÙŠ Ø§Ù„Ø£Ø±Ø¯Ù†.\n[CLS]: O\nØ²Ø§Ø±: O\nØ§Ù„Ø³Ø§: O\n##ÙŠØ­: O\n##ÙˆÙ†: O\nÙ…Ø¯ÙŠÙ†Ø©: O\nØ§Ù„Ø¨ØªØ±: B-GPE\n##Ø§Ø¡: L-GPE\nØ§Ù„Ø§Ø«Ø±ÙŠØ©: O\nÙÙŠ: O\nØ§Ù„Ø§Ø±Ø¯Ù†: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ÙŠÙ‚Ø¯Ù… Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„ÙˆØ·Ù†ÙŠ Ø®Ø¯Ù…Ø§Øª Ø·Ø¨ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©.\n[CLS]: O\nÙŠÙ‚Ø¯Ù…: O\nØ§Ù„Ù…Ø³ØªØ´ÙÙ‰: B-ORG\nØ§Ù„ÙˆØ·Ù†ÙŠ: L-ORG\nØ®Ø¯Ù…Ø§Øª: O\nØ·Ø¨ÙŠØ©: O\nØ¹Ø§Ù„ÙŠØ©: O\nØ§Ù„Ø¬ÙˆØ¯Ø©: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ØªÙ… Ø¹Ø±Ø¶ Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¹Ù„Ù‰ Ù‚Ù†Ø§Ø© Ø§Ù„Ø¬Ø²ÙŠØ±Ø©.\n[CLS]: O\nØªÙ…: O\nØ¹Ø±Ø¶: O\nØ§Ù„ÙÙŠÙ„Ù…: O\nØ§Ù„ÙˆØ«Ø§ÙŠÙ‚: O\n##ÙŠ: O\nØ§Ù„Ø¬Ø¯ÙŠØ¯: O\nØ¹Ù„Ù‰: O\nÙ‚Ù†Ø§Ø©: B-ORG\nØ§Ù„Ø¬Ø²ÙŠØ±Ø©: L-ORG\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: Ø§ÙØªØªØ­Øª Ø´Ø±ÙƒØ© Ø³Ø§Ù…Ø³ÙˆÙ†Ø¬ ÙØ±Ø¹Ø§Ù‹ Ø¬Ø¯ÙŠØ¯Ø§Ù‹ ÙÙŠ Ø¯Ø¨ÙŠ.\n[CLS]: O\nØ§ÙØªØªØ­Øª: O\nØ´Ø±ÙƒØ©: O\nØ³Ø§Ù…Ø³ÙˆÙ†Ø¬: B-ORG\nÙØ±Ø¹: O\n##Ø§: O\nØ¬Ø¯ÙŠØ¯Ø§: O\nÙÙŠ: O\nØ¯Ø¨ÙŠ: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: Ø³ÙŠØªÙ… ØªØ³Ù„ÙŠÙ… Ø§Ù„Ø¬ÙˆØ§Ø¦Ø² ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³ Ø§Ù„Ù‚Ø§Ø¯Ù….\n[CLS]: O\nØ³ÙŠØªÙ…: O\nØªØ³Ù„ÙŠÙ…: O\nØ§Ù„Ø¬ÙˆØ§ÙŠØ²: O\nÙŠÙˆÙ…: B-TIMEX\nØ§Ù„Ø®Ù…ÙŠØ³: L-TIMEX\nØ§Ù„Ù‚Ø§Ø¯Ù…: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: Ø£Ø­Ø¨ Ù‚Ø±Ø§Ø¡Ø© ÙƒØªØ§Ø¨ 'Ù…Ø¦Ø© Ø¹Ø§Ù… Ù…Ù† Ø§Ù„Ø¹Ø²Ù„Ø©'.\n[CLS]: O\nØ§Ø­Ø¨: O\nÙ‚Ø±Ø§Ø¡Ø©: O\nÙƒØªØ§Ø¨: O\n': O\nÙ…ÙŠØ©: B-WOA\nØ¹Ø§Ù…: I-WOA\nÙ…Ù†: I-WOA\nØ§Ù„Ø¹Ø²Ù„: L-WOA\n##Ø©: L-WOA\n': O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ØªØ¹ÙŠÙ‘Ù† Ø§Ù„Ø¯ÙƒØªÙˆØ± ÙŠÙˆØ³Ù Ø¹Ù…ÙŠØ¯Ø§Ù‹ Ù„ÙƒÙ„ÙŠØ© Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©.\n[CLS]: O\nØªØ¹ÙŠÙ†: O\nØ§Ù„Ø¯ÙƒØªÙˆØ±: B-TTL\nÙŠÙˆØ³Ù: B-PER\nØ¹Ù…ÙŠØ¯: O\n##Ø§: O\nÙ„ÙƒÙ„: B-ORG\n##ÙŠØ©: B-ORG\nØ§Ù„Ù‡Ù†Ø¯Ø³Ø©: L-ORG\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ø³Ø¨Ø§Ù†ÙŠØ© Ù…Ù†ØªØ´Ø±Ø© ÙÙŠ Ù‚Ø§Ø±Ø© Ø£Ù…Ø±ÙŠÙƒØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ©.\n[CLS]: O\nØ§Ù„Ù„ØºØ©: B-ANG\nØ§Ù„Ø§Ø³Ø¨Ø§Ù†ÙŠØ©: L-ANG\nÙ…Ù†ØªØ´Ø±Ø©: O\nÙÙŠ: O\nÙ‚Ø§Ø±Ø©: O\nØ§Ù…Ø±ÙŠÙƒØ§: B-LOC\nØ§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ©: L-LOC\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: Ø²Ø§Ø± Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ø±Ø¦Ø§Ø³ÙŠ Ù‚ØµØ± Ù‚Ø±Ø·Ø§Ø¬ Ø§Ù„Ø±Ø¦Ø§Ø³ÙŠ ÙÙŠ ØªÙˆÙ†Ø³.\n[CLS]: O\nØ²Ø§Ø±: O\nØ§Ù„ÙØ±ÙŠÙ‚: O\nØ§Ù„Ø±ÙŠØ§Ø³ÙŠ: O\nÙ‚ØµØ±: B-FAC\nÙ‚Ø±Ø·Ø§Ø¬: L-FAC\nØ§Ù„Ø±ÙŠØ§Ø³ÙŠ: L-FAC\nÙÙŠ: O\nØªÙˆÙ†Ø³: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ÙŠØ¨Ø¯Ø£ Ù…Ù‡Ø±Ø¬Ø§Ù† ÙƒØ§Ù† Ø§Ù„Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ ÙÙŠ Ù…Ø§ÙŠÙˆ Ù…Ù† ÙƒÙ„ Ø¹Ø§Ù….\n[CLS]: O\nÙŠØ¨Ø¯Ø§: O\nÙ…Ù‡Ø±Ø¬Ø§Ù†: B-EVE\nÙƒØ§Ù†: L-EVE\nØ§Ù„Ø³ÙŠÙ†Ù…Ø§ÙŠÙŠ: O\nÙÙŠ: O\nÙ…Ø§ÙŠÙˆ: B-TIMEX\nÙ…Ù†: O\nÙƒÙ„: O\nØ¹Ø§Ù…: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ØªØ¹Ø¯ Ø¬Ø¨Ø§Ù„ Ø§Ù„Ù‡ÙŠÙ…Ø§Ù„Ø§ÙŠØ§ Ù…Ù† Ø£Ø¹Ù„Ù‰ Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø¬Ø¨Ø§Ù„ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù….\n[CLS]: O\nØªØ¹Ø¯: O\nØ¬Ø¨Ø§Ù„: B-LOC\nØ§Ù„Ù‡ÙŠ: L-LOC\n##Ù…Ø§Ù„: L-LOC\n##Ø§ÙŠØ§: L-LOC\nÙ…Ù†: O\nØ§Ø¹Ù„Ù‰: O\nØ³Ù„Ø§Ø³Ù„: O\nØ§Ù„Ø¬Ø¨Ø§Ù„: O\nÙÙŠ: O\nØ§Ù„Ø¹Ø§Ù„Ù…: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ØªÙ… Ø¥Ø¹Ù„Ø§Ù† Ø§Ù„ÙØ§Ø¦Ø² Ø¨Ø¬Ø§Ø¦Ø²Ø© Ù†ÙˆØ¨Ù„ Ù„Ù„Ø³Ù„Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹.\n[CLS]: O\nØªÙ…: O\nØ§Ø¹Ù„Ø§Ù†: O\nØ§Ù„ÙØ§ÙŠØ²: B-PER\nØ¨Ø¬Ø§ÙŠØ²Ø©: O\nÙ†ÙˆØ¨Ù„: B-MISC\nÙ„Ù„Ø³Ù„Ø§Ù…: L-MISC\nÙ‡Ø°Ø§: O\nØ§Ù„Ø§Ø³Ø¨ÙˆØ¹: O\n.: O\n[SEP]: O\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import os\n\n# Step 1: Save the model and tokenizer\nsave_directory = \"/kaggle/working/FinalModel\"\nos.makedirs(save_directory, exist_ok=True)\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\nprint(f\"âœ… Model and tokenizer have been saved to {save_directory}\")\n\n# Step 2: Zip the directory\n!zip -r /kaggle/working/FinalModel.zip /kaggle/working/FinalModel\nprint(\"âœ… Model has been zipped successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:04:21.526810Z","iopub.execute_input":"2025-05-12T21:04:21.527042Z","iopub.status.idle":"2025-05-12T21:04:45.081050Z","shell.execute_reply.started":"2025-05-12T21:04:21.527022Z","shell.execute_reply":"2025-05-12T21:04:45.080321Z"}},"outputs":[{"name":"stdout","text":"âœ… Model and tokenizer have been saved to /kaggle/working/FinalModel\n  adding: kaggle/working/FinalModel/ (stored 0%)\n  adding: kaggle/working/FinalModel/model.safetensors (deflated 7%)\n  adding: kaggle/working/FinalModel/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/FinalModel/config.json (deflated 63%)\n  adding: kaggle/working/FinalModel/vocab.txt (deflated 63%)\n  adding: kaggle/working/FinalModel/tokenizer.json (deflated 73%)\n  adding: kaggle/working/FinalModel/tokenizer_config.json (deflated 74%)\nâœ… Model has been zipped successfully!\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Ø±Ø§Ø¨Ø· Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ø¨Ø§Ø´Ø±Ø©\nFileLink('/kaggle/working/FinalModel.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:04:45.082012Z","iopub.execute_input":"2025-05-12T21:04:45.082314Z","iopub.status.idle":"2025-05-12T21:04:45.087858Z","shell.execute_reply.started":"2025-05-12T21:04:45.082286Z","shell.execute_reply":"2025-05-12T21:04:45.087100Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/FinalModel.zip","text/html":"<a href='/kaggle/working/FinalModel.zip' target='_blank'>/kaggle/working/FinalModel.zip</a><br>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}