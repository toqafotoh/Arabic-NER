{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ⭐ Arabic Named Entity Recognition (NER) Project\n\n### Project Overview\nIn this project, we'll be developing a Named Entity Recognition (NER) system for Arabic text to identify entities such as:\n- **Persons (PER)**\n- **Organizations (ORG)**\n- **Locations (LOC)**\n- **Dates (TIMEX)**\n---\n\n### Tools and Libraries:\n- HuggingFace Transformers\n- PyTorch\n- Kaggle Datasets\n- Tokenizers\n- Scikit-Learn","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:52.511448Z","iopub.execute_input":"2025-05-12T18:52:52.511776Z","iopub.status.idle":"2025-05-12T18:52:52.515836Z","shell.execute_reply.started":"2025-05-12T18:52:52.511753Z","shell.execute_reply":"2025-05-12T18:52:52.514930Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:52.517993Z","iopub.execute_input":"2025-05-12T18:52:52.518186Z","iopub.status.idle":"2025-05-12T18:52:55.690992Z","shell.execute_reply.started":"2025-05-12T18:52:52.518160Z","shell.execute_reply":"2025-05-12T18:52:55.690223Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Login using e.g. `huggingface-cli login` to access this dataset\ndataset = load_dataset(\"iahlt/arabic_ner_mafat\")\ntrain_dataset = dataset[\"train\"]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:55.692807Z","iopub.execute_input":"2025-05-12T18:52:55.693060Z","iopub.status.idle":"2025-05-12T18:52:56.704766Z","shell.execute_reply.started":"2025-05-12T18:52:55.693038Z","shell.execute_reply":"2025-05-12T18:52:56.704215Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:56.705392Z","iopub.execute_input":"2025-05-12T18:52:56.705573Z","iopub.status.idle":"2025-05-12T18:52:56.710428Z","shell.execute_reply.started":"2025-05-12T18:52:56.705560Z","shell.execute_reply":"2025-05-12T18:52:56.709676Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'raw_tags', 'ner_tags', 'spaces', 'spans', 'record', 'text'],\n        num_rows: 40000\n    })\n})"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# Extract all the tags\nall_tags = set(tag for example in dataset[\"train\"]['raw_tags'] for tag in example)\n\n# Create a mapping from tag name to number\ntag2id = {tag: idx for idx, tag in enumerate(sorted(all_tags))}\nid2tag = {v: k for k, v in tag2id.items()}\n\n# Add new columns with numbers instead of raw_tags\ndef convert_tags(example):\n    example['labels'] = [tag2id[tag] for tag in example['raw_tags']]\n    return example\n\n# Apply the conversion to the entire dataset\ndataset = dataset.map(convert_tags)\n\n# If you want to see the mapping\nprint(\"Tag2ID:\", tag2id)\nprint(\"ID2Tag:\", id2tag)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:56.712107Z","iopub.execute_input":"2025-05-12T18:52:56.712330Z","iopub.status.idle":"2025-05-12T18:52:58.366817Z","shell.execute_reply.started":"2025-05-12T18:52:56.712314Z","shell.execute_reply":"2025-05-12T18:52:58.366216Z"}},"outputs":[{"name":"stdout","text":"Tag2ID: {'B-ANG': 0, 'B-DUC': 1, 'B-EVE': 2, 'B-FAC': 3, 'B-GPE': 4, 'B-INFORMAL': 5, 'B-LOC': 6, 'B-MISC': 7, 'B-ORG': 8, 'B-PER': 9, 'B-TIMEX': 10, 'B-TTL': 11, 'B-WOA': 12, 'I-ANG': 13, 'I-DUC': 14, 'I-EVE': 15, 'I-FAC': 16, 'I-GPE': 17, 'I-INFORMAL': 18, 'I-LOC': 19, 'I-MISC': 20, 'I-ORG': 21, 'I-PER': 22, 'I-TIMEX': 23, 'I-TTL': 24, 'I-WOA': 25, 'L-ANG': 26, 'L-DUC': 27, 'L-EVE': 28, 'L-FAC': 29, 'L-GPE': 30, 'L-INFORMAL': 31, 'L-LOC': 32, 'L-MISC': 33, 'L-ORG': 34, 'L-PER': 35, 'L-TIMEX': 36, 'L-TTL': 37, 'L-WOA': 38, 'O': 39}\nID2Tag: {0: 'B-ANG', 1: 'B-DUC', 2: 'B-EVE', 3: 'B-FAC', 4: 'B-GPE', 5: 'B-INFORMAL', 6: 'B-LOC', 7: 'B-MISC', 8: 'B-ORG', 9: 'B-PER', 10: 'B-TIMEX', 11: 'B-TTL', 12: 'B-WOA', 13: 'I-ANG', 14: 'I-DUC', 15: 'I-EVE', 16: 'I-FAC', 17: 'I-GPE', 18: 'I-INFORMAL', 19: 'I-LOC', 20: 'I-MISC', 21: 'I-ORG', 22: 'I-PER', 23: 'I-TIMEX', 24: 'I-TTL', 25: 'I-WOA', 26: 'L-ANG', 27: 'L-DUC', 28: 'L-EVE', 29: 'L-FAC', 30: 'L-GPE', 31: 'L-INFORMAL', 32: 'L-LOC', 33: 'L-MISC', 34: 'L-ORG', 35: 'L-PER', 36: 'L-TIMEX', 37: 'L-TTL', 38: 'L-WOA', 39: 'O'}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# 🏷️ Tag2ID Mappings for Arabic NER Dataset\n\nThe table below explains each NER tag, its meaning, along with multiple Arabic examples to cover various cases:\n\n<table style=\"border-collapse: collapse; width: 100%; text-align: center;\">\n  <thead style=\"background-color: #f2f2f2; color: #333;\">\n    <tr>\n      <th style=\"padding: 10px; border: 1px solid #ccc;\">Tag</th>\n      <th style=\"padding: 10px; border: 1px solid #ccc;\">Meaning</th>\n      <th style=\"padding: 10px; border: 1px solid #ccc;\">Arabic Examples</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">PER</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Person 👤</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"أحمد\"، \"فاطمة\"، \"محمد صلاح\"، \"نجيب محفوظ\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">ORG</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Organization 🏢</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"شركة الاتصالات\"، \"اليونيسكو\"، \"جامعة القاهرة\"، \"نادي الأهلي\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">LOC</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Location 📍</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"الحديقة\"، \"النهر\"، \"الصحراء\"، \"الجبل\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">GPE</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Geopolitical Entity 🌍</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"مصر\"، \"السعودية\"، \"القاهرة\"، \"الرياض\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">TIMEX</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Time Expression ⏰</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"الساعة\"، \"اليوم\"، \"عام ٢٠٢٠\"، \"منتصف الليل\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">TTL</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Title 📚</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"عنوان الكتاب\"، \"رواية الأسود يليق بك\"، \"فيلم الكنز\"، \"مقالة علمية\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">WOA</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Work of Art 🎨</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"لوحة الموناليزا\"، \"تمثال الحرية\"، \"قصيدة الأطلال\"، \"مقطوعة موسيقية\"</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">MISC</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">Miscellaneous 📦</td>\n      <td style=\"padding: 10px; border: 1px solid #ccc;\">\"موقع إلكتروني\"، \"اسم حدث\"، \"مصطلح علمي\"، \"براءة اختراع\"</td>\n    </tr>\n  </tbody>\n</table>\n\n> This table gives a richer and clearer view 🌟 of each tag with several examples to better understand how tags are used in Arabic contexts.\n","metadata":{}},{"cell_type":"code","source":"dataset[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.367479Z","iopub.execute_input":"2025-05-12T18:52:58.367681Z","iopub.status.idle":"2025-05-12T18:52:58.373290Z","shell.execute_reply.started":"2025-05-12T18:52:58.367666Z","shell.execute_reply":"2025-05-12T18:52:58.372594Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'tokens': ['البروفسور', 'محمود', 'خليل'],\n 'raw_tags': ['B-TTL', 'B-PER', 'L-PER'],\n 'ner_tags': [47, 38, 37],\n 'spaces': [1, 1, 0],\n 'spans': [{'end': 9, 'label': 'TTL', 'start': 0, 'text': 'البروفسور'},\n  {'end': 20, 'label': 'PER', 'start': 10, 'text': 'محمود خليل'}],\n 'record': '{\"metadata\": {\"doc_id\": \"003875358633647be47857dcee7869cfc03720431774756bd997a7cf87dd93bb\", \"url\": \"https://www.alarab.com//Article/824736\", \"source\": \"AlArab\", \"title\": \"كلية سخنين لتأهيل المعلمين تحصل على اعتراف مجلس التعليم العالي بإعطاء لقب M.Teach\", \"authors\": \"موقع العرب وصحيفة كل العرب- الناصرة\", \"date\": \"2017-09-14 15:59:34\", \"domains\": \"Students\", \"parnumber\": \"8\", \"sentnumber\": \"none\"}, \"text\": \"البروفسور محمود خليل\", \"label\": [[0, 9, \"TTL\"], [10, 20, \"PER\"]], \"user\": \"nlhowell\", \"timestamp\": 1685356355.6331542, \"flatten\": {\"tokens\": [\"البروفسور\", \"محمود\", \"خليل\"], \"ner_tags\": [\"B-TTL\", \"B-PER\", \"L-PER\"], \"spaces\": [1, 1, 0]}, \"label_hierarchy\": {\"0\": [{\"end\": 9, \"label\": \"TTL\", \"start\": 0, \"text\": \"البروفسور\"}, {\"end\": 20, \"label\": \"PER\", \"start\": 10, \"text\": \"محمود خليل\"}], \"1\": null, \"2\": null, \"3\": null, \"4\": null, \"5\": null}, \"has_overlappings\": false, \"n_hierarchy_levels\": 1}',\n 'text': 'البروفسور محمود خليل',\n 'labels': [11, 9, 35]}"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from datasets import DatasetDict\n\ndef simplify(example):\n    return {\n        \"tokens\": example[\"tokens\"],\n        \"labels\": example[\"labels\"]\n    }\n\nkeep_cols = [\"tokens\", \"labels\"]\n\nsimplified_dataset = DatasetDict({\n    split: dataset[split]\n        .map(simplify, remove_columns=[col for col in dataset[split].column_names if col not in keep_cols])\n    for split in dataset\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.373987Z","iopub.execute_input":"2025-05-12T18:52:58.374206Z","iopub.status.idle":"2025-05-12T18:52:58.392209Z","shell.execute_reply.started":"2025-05-12T18:52:58.374182Z","shell.execute_reply":"2025-05-12T18:52:58.391629Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"simplified_dataset[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.392833Z","iopub.execute_input":"2025-05-12T18:52:58.393091Z","iopub.status.idle":"2025-05-12T18:52:58.406321Z","shell.execute_reply.started":"2025-05-12T18:52:58.393068Z","shell.execute_reply":"2025-05-12T18:52:58.405743Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'tokens': ['البروفسور', 'محمود', 'خليل'], 'labels': [11, 9, 35]}"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"# 🥰 Dataset Preprocessing Summary\n\nIn this project, we focused on preprocessing the **Arabic NER** dataset by following these steps:\n\n### 1. **Selecting Important Columns**  \nWe selected only the necessary columns: **tokens** and **labels**. This helps to focus on the key information needed for Named Entity Recognition (NER).\n\n### 2. **Tag Mapping**  \nWe created a mapping between the tag names and numerical values:\n- **Tag to ID Mapping**: We assigned a unique number to each tag.\n- **ID to Tag Mapping**: We reversed the mapping to get back from IDs to tags.\n\n### 3. **Unified Tagging**  \nWe replaced the original **raw_tags** with numerical labels corresponding to each entity type, making it easier to process the data for training.\n\n---\n\nWith these steps, the dataset is now ready for model training and optimized for NER tasks! 🚀\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n\nnum_labels = 40\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"asafaya/bert-base-arabic\",\n    num_labels=num_labels,\n    id2label=id2tag,\n    label2id=tag2id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.406848Z","iopub.execute_input":"2025-05-12T18:52:58.407002Z","iopub.status.idle":"2025-05-12T18:52:58.727258Z","shell.execute_reply.started":"2025-05-12T18:52:58.406989Z","shell.execute_reply":"2025-05-12T18:52:58.726645Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"\n# ✨ Tokenization and Label Alignment for Arabic NER\n\nTo prepare the Arabic NER dataset for model training, we apply a tokenization and label alignment step.  \nThe goal is to **tokenize** each word properly and **align** the NER labels with the corresponding tokens, taking into account that some words may be split into multiple sub-tokens.\n\n---\n\n### 🔹 Step Explanation:\n\n- 1️⃣ **Tokenization:** Tokenize each example separately using the pretrained tokenizer.\n- 2️⃣ **Tracking:** Keep track of which token belongs to which original word (`word_ids`).\n- 3️⃣ **Label Assignment:** Assign the correct label only to the first token of each word. For sub-tokens or special tokens, assign `-100` (to ignore them during loss calculation).\n- 4️⃣ **Store Labels:** Attach the aligned labels back into the tokenized output.\n- 5️⃣ **Batch Processing:** Apply the mapping to the entire dataset using `batched=True` for efficiency.\n\n---\n\n### 🎯 Why This Matters?\n\n- **Subword Tokenization:** In languages like Arabic, tokenizers (such as BERT-based ones) often split words into multiple subwords.\n- **Correct Label Alignment:** Ensures only the first sub-token carries the label, avoiding confusion during model training.\n- **Loss Masking:** Using `-100` masks irrelevant positions, helping the model focus on learning the right targets.\n","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"],\n        truncation=True,\n        is_split_into_words=True,\n        padding=True,\n        max_length=512\n    )\n\n    all_labels = []\n    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n        labels = []\n        previous_word_idx = None\n        for word_idx in word_ids:\n            if word_idx is None:\n                labels.append(-100)\n            elif word_idx != previous_word_idx:\n                labels.append(examples[\"labels\"][i][word_idx])\n            else:\n                labels.append(-100)\n            previous_word_idx = word_idx\n        all_labels.append(labels)\n\n    tokenized_inputs[\"labels\"] = all_labels\n    return tokenized_inputs\n\n\ntokenized_datasets = simplified_dataset.map(tokenize_and_align_labels, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.727861Z","iopub.execute_input":"2025-05-12T18:52:58.728050Z","iopub.status.idle":"2025-05-12T18:52:58.745608Z","shell.execute_reply.started":"2025-05-12T18:52:58.728036Z","shell.execute_reply":"2025-05-12T18:52:58.744875Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:52:58.747976Z","iopub.execute_input":"2025-05-12T18:52:58.748320Z","iopub.status.idle":"2025-05-12T18:53:01.856567Z","shell.execute_reply.started":"2025-05-12T18:52:58.748303Z","shell.execute_reply":"2025-05-12T18:53:01.855611Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!pip install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:01.857577Z","iopub.execute_input":"2025-05-12T18:53:01.857812Z","iopub.status.idle":"2025-05-12T18:53:04.758363Z","shell.execute_reply.started":"2025-05-12T18:53:01.857791Z","shell.execute_reply":"2025-05-12T18:53:04.757594Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"\n# 📊 Evaluation Setup for Arabic NER\n\nTo properly evaluate our NER model performance, we set up the data collator, evaluation metric, and metric computation function.\n\n---\n\n### 🔹 Key Steps:\n\n- **Data Collation:**  \n  Use `DataCollatorForTokenClassification` to dynamically pad inputs and labels during training.\n\n- **Metric Loading:**  \n  Load the `seqeval` metric, specialized for sequence labeling tasks like NER.\n\n- **Metric Computation:**  \n  - Predict the best label for each token using `argmax`.\n  - Align predictions and labels, ignoring padding tokens (`-100`).\n  - Calculate overall **precision**, **recall**, **f1-score**, and **accuracy**.\n\n \n","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\nimport numpy as np\nimport evaluate\n\n# Data collator\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\n# Metrics\nmetric = evaluate.load(\"seqeval\")\n\ndef compute_metrics(p):\n    predictions, labels = p\n    # Ensure that predictions is an array of probabilities, and if so, use np.argmax to choose the label with the highest probability\n    predictions = np.argmax(predictions, axis=2)\n\n    # Get the true predictions (selected based on the highest probability)\n    true_predictions = [\n        [id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    # Get the true labels (from labels, removing the -100 values)\n    true_labels = [\n        [id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    # Calculate the results using the seqeval library\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n\n    # Return the evaluation metrics\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:04.759375Z","iopub.execute_input":"2025-05-12T18:53:04.759604Z","iopub.status.idle":"2025-05-12T18:53:05.263307Z","shell.execute_reply.started":"2025-05-12T18:53:04.759569Z","shell.execute_reply":"2025-05-12T18:53:05.262713Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torch\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:05.263993Z","iopub.execute_input":"2025-05-12T18:53:05.264196Z","iopub.status.idle":"2025-05-12T18:53:05.459669Z","shell.execute_reply.started":"2025-05-12T18:53:05.264158Z","shell.execute_reply":"2025-05-12T18:53:05.458942Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=40, bias=True)\n)"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"import os\nimport warnings\nimport logging\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:05.460509Z","iopub.execute_input":"2025-05-12T18:53:05.460786Z","iopub.status.idle":"2025-05-12T18:53:05.466269Z","shell.execute_reply.started":"2025-05-12T18:53:05.460763Z","shell.execute_reply":"2025-05-12T18:53:05.465561Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"\n# 🚀 Model Training Setup for Arabic NER\n\nIn this section, we use the **Trainer API** from Hugging Face Transformers to train the model **CAMeL-Lab's bert-base-arabic-camelbert-ca** (`camelbert`) on the **Arabic NER dataset**.\n\n\n\n### 🔹 Training Arguments Explanation:\n\n- **output_dir=\"/kaggle/working/\"**: Specifies the directory where model checkpoints and training results will be saved.\n- **eval_strategy=\"epoch\"**: Evaluates the model at the end of each *epoch*.\n- **save_strategy=\"epoch\"**: Saves the model after each *epoch*.\n- **learning_rate=2e-5**: Sets the learning rate for the optimizer.\n- **per_device_train_batch_size=16**: Batch size per device (CPU/GPU) during training.\n- **per_device_eval_batch_size=16**: Batch size per device during evaluation.\n- **num_train_epochs=3**: Number of *epochs* for training, set to 3 here.\n- **weight_decay=0.01**: Applies L2 regularization to prevent overfitting.\n- **disable_tqdm=False**: Enables the progress bar during training.\n- **gradient_checkpointing=True**: Saves memory by recomputing activations during the backward pass (useful for large models).\n- **dataloader_num_workers=4**: Number of workers for data loading to speed up the process.\n\n\n\n### 🔹 Trainer Setup Explanation:\n\n- **model=model.to(device)**: Moves the model to the appropriate device (CPU or GPU).\n- **args=training_args**: Uses the specified training arguments defined earlier.\n- **train_dataset=tokenized_datasets[\"train\"]**: The dataset used for training after preprocessing.\n- **eval_dataset=tokenized_datasets[\"train\"].select(range(1000))**: A subset of 1000 examples for evaluation.\n- **tokenizer=tokenizer**: The tokenizer used to convert text into tokens.\n- **data_collator=data_collator**: Responsible for padding the inputs to the correct size during batching.\n- **compute_metrics=compute_metrics**: A function that calculates Precision, Recall, F1, and overall Accuracy.\n\n\n\n### 🛠️ Final Step: Training\n\nFinally, we train the model using `trainer.train()`, which starts the training process on the **Arabic NER dataset**.\n\n\n","metadata":{}},{"cell_type":"code","source":"tokenized_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\nval_test_split = tokenized_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n\ntrain_dataset = tokenized_split[\"train\"]\nvalid_dataset = val_test_split[\"train\"]\ntest_dataset = val_test_split[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:05.466963Z","iopub.execute_input":"2025-05-12T18:53:05.467266Z","iopub.status.idle":"2025-05-12T18:53:05.491226Z","shell.execute_reply.started":"2025-05-12T18:53:05.467211Z","shell.execute_reply":"2025-05-12T18:53:05.490576Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom transformers import Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    disable_tqdm=False,\n    gradient_checkpointing=True\n)\n\ntrainer = Trainer(\n    model=model.to(device),\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T18:53:05.492329Z","iopub.execute_input":"2025-05-12T18:53:05.492591Z","iopub.status.idle":"2025-05-12T21:03:14.069088Z","shell.execute_reply.started":"2025-05-12T18:53:05.492568Z","shell.execute_reply":"2025-05-12T21:03:14.068357Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6000/6000 2:10:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.190300</td>\n      <td>0.179529</td>\n      <td>0.746202</td>\n      <td>0.800179</td>\n      <td>0.772249</td>\n      <td>0.948414</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.139800</td>\n      <td>0.166288</td>\n      <td>0.761528</td>\n      <td>0.813257</td>\n      <td>0.786543</td>\n      <td>0.951965</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.106800</td>\n      <td>0.173279</td>\n      <td>0.767163</td>\n      <td>0.820879</td>\n      <td>0.793112</td>\n      <td>0.953025</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6000, training_loss=0.17119370396931965, metrics={'train_runtime': 7807.3911, 'train_samples_per_second': 12.296, 'train_steps_per_second': 0.769, 'total_flos': 2.495136155267712e+16, 'train_loss': 0.17119370396931965, 'epoch': 3.0})"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"results = trainer.evaluate(eval_dataset=test_dataset)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:03:14.069938Z","iopub.execute_input":"2025-05-12T21:03:14.070142Z","iopub.status.idle":"2025-05-12T21:04:21.341770Z","shell.execute_reply.started":"2025-05-12T21:03:14.070127Z","shell.execute_reply":"2025-05-12T21:04:21.340913Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 01:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.17174355685710907, 'eval_precision': 0.7637311620598529, 'eval_recall': 0.8175076452599388, 'eval_f1': 0.7897049591964846, 'eval_accuracy': 0.9523553162853298, 'eval_runtime': 67.229, 'eval_samples_per_second': 59.498, 'eval_steps_per_second': 3.719, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from torchinfo import summary\n\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Trainable parameters: {trainable_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:17:07.257032Z","iopub.execute_input":"2025-05-12T21:17:07.257643Z","iopub.status.idle":"2025-05-12T21:17:07.283276Z","shell.execute_reply.started":"2025-05-12T21:17:07.257617Z","shell.execute_reply":"2025-05-12T21:17:07.282646Z"}},"outputs":[{"name":"stdout","text":"Total parameters: 110057512\nTrainable parameters: 110057512\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"\n\n# 📝 Test Sentences for Arabic NER\n\nThis section includes a set of test sentences used to evaluate the performance of the Arabic NER model.\n","metadata":{}},{"cell_type":"code","source":"test_sentences_ner = [\n    \"سافر أحمد إلى القاهرة لحضور مؤتمر التكنولوجيا.\",\n    \"مُنح كتاب 'الخيميائي' جائزة أفضل رواية.\",\n    \"ستقام المباراة النهائية في ملعب الملك فهد الدولي.\",\n    \"تعمل ليلى في شركة مايكروسوفت منذ خمس سنوات.\",\n    \"ولد العالم إسحاق نيوتن في إنجلترا.\",\n    \"أقيمت فعاليات معرض الكتاب الدولي في الرياض.\",\n    \"يبدأ العام الدراسي الجديد في سبتمبر.\",\n    \"يتحدث سامي اللغة الفرنسية بطلاقة.\",\n    \"زار السائحون مدينة البتراء الأثرية في الأردن.\",\n    \"يقدم المستشفى الوطني خدمات طبية عالية الجودة.\",\n    \"تم عرض الفيلم الوثائقي الجديد على قناة الجزيرة.\",\n    \"افتتحت شركة سامسونج فرعاً جديداً في دبي.\",\n    \"سيتم تسليم الجوائز يوم الخميس القادم.\",\n    \"أحب قراءة كتاب 'مئة عام من العزلة'.\",\n    \"تعيّن الدكتور يوسف عميداً لكلية الهندسة.\",\n    \"اللغة الإسبانية منتشرة في قارة أمريكا الجنوبية.\",\n    \"زار الفريق الرئاسي قصر قرطاج الرئاسي في تونس.\",\n    \"يبدأ مهرجان كان السينمائي في مايو من كل عام.\",\n    \"تعد جبال الهيمالايا من أعلى سلاسل الجبال في العالم.\",\n    \"تم إعلان الفائز بجائزة نوبل للسلام هذا الأسبوع.\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:04:21.342672Z","iopub.execute_input":"2025-05-12T21:04:21.343434Z","iopub.status.idle":"2025-05-12T21:04:21.347475Z","shell.execute_reply.started":"2025-05-12T21:04:21.343411Z","shell.execute_reply":"2025-05-12T21:04:21.346737Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"for sentence in test_sentences_ner:\n    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    logits_cpu = logits.cpu().numpy()\n    predictions = np.argmax(logits_cpu, axis=2)\n    predicted_tags = [id2tag[pred] for pred in predictions[0]]\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n\n    print(f\"Sentence: {sentence}\")\n    for token, tag in zip(tokens, predicted_tags):\n        print(f\"{token}: {tag}\")\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:04:21.348200Z","iopub.execute_input":"2025-05-12T21:04:21.348376Z","iopub.status.idle":"2025-05-12T21:04:21.526158Z","shell.execute_reply.started":"2025-05-12T21:04:21.348363Z","shell.execute_reply":"2025-05-12T21:04:21.525575Z"}},"outputs":[{"name":"stdout","text":"Sentence: سافر أحمد إلى القاهرة لحضور مؤتمر التكنولوجيا.\n[CLS]: O\nسافر: O\nاحمد: B-PER\nالى: O\nالقاهرة: B-GPE\nلحضور: O\nموتمر: B-EVE\nالتكنولوجيا: L-EVE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: مُنح كتاب 'الخيميائي' جائزة أفضل رواية.\n[CLS]: O\nمنح: O\nكتاب: O\n': O\nالخي: B-WOA\n##مي: L-WOA\n##ايي: L-WOA\n': O\nجايزة: O\nافضل: O\nرواية: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ستقام المباراة النهائية في ملعب الملك فهد الدولي.\n[CLS]: O\nستقام: O\nالمباراة: O\nالنهايية: O\nفي: O\nملعب: B-FAC\nالملك: I-FAC\nفهد: I-FAC\nالدولي: L-FAC\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: تعمل ليلى في شركة مايكروسوفت منذ خمس سنوات.\n[CLS]: O\nتعمل: O\nليلى: B-PER\nفي: O\nشركة: O\nمايكروسوفت: B-ORG\nمنذ: O\nخمس: O\nسنوات: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: ولد العالم إسحاق نيوتن في إنجلترا.\n[CLS]: O\nولد: O\nالعالم: B-TTL\nاسحاق: B-PER\nنيوت: L-PER\n##ن: L-PER\nفي: O\nانجلترا: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: أقيمت فعاليات معرض الكتاب الدولي في الرياض.\n[CLS]: O\nاقيمت: O\nفعاليات: O\nمعرض: B-EVE\nالكتاب: I-EVE\nالدولي: L-EVE\nفي: O\nالرياض: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: يبدأ العام الدراسي الجديد في سبتمبر.\n[CLS]: O\nيبدا: O\nالعام: O\nالدراسي: O\nالجديد: O\nفي: O\nسبتمبر: B-TIMEX\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: يتحدث سامي اللغة الفرنسية بطلاقة.\n[CLS]: O\nيتحدث: O\nسامي: B-PER\nاللغة: B-ANG\nالفرنسية: L-ANG\nبطل: O\n##اقة: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: زار السائحون مدينة البتراء الأثرية في الأردن.\n[CLS]: O\nزار: O\nالسا: O\n##يح: O\n##ون: O\nمدينة: O\nالبتر: B-GPE\n##اء: L-GPE\nالاثرية: O\nفي: O\nالاردن: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: يقدم المستشفى الوطني خدمات طبية عالية الجودة.\n[CLS]: O\nيقدم: O\nالمستشفى: B-ORG\nالوطني: L-ORG\nخدمات: O\nطبية: O\nعالية: O\nالجودة: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: تم عرض الفيلم الوثائقي الجديد على قناة الجزيرة.\n[CLS]: O\nتم: O\nعرض: O\nالفيلم: O\nالوثايق: O\n##ي: O\nالجديد: O\nعلى: O\nقناة: B-ORG\nالجزيرة: L-ORG\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: افتتحت شركة سامسونج فرعاً جديداً في دبي.\n[CLS]: O\nافتتحت: O\nشركة: O\nسامسونج: B-ORG\nفرع: O\n##ا: O\nجديدا: O\nفي: O\nدبي: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: سيتم تسليم الجوائز يوم الخميس القادم.\n[CLS]: O\nسيتم: O\nتسليم: O\nالجوايز: O\nيوم: B-TIMEX\nالخميس: L-TIMEX\nالقادم: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: أحب قراءة كتاب 'مئة عام من العزلة'.\n[CLS]: O\nاحب: O\nقراءة: O\nكتاب: O\n': O\nمية: B-WOA\nعام: I-WOA\nمن: I-WOA\nالعزل: L-WOA\n##ة: L-WOA\n': O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: تعيّن الدكتور يوسف عميداً لكلية الهندسة.\n[CLS]: O\nتعين: O\nالدكتور: B-TTL\nيوسف: B-PER\nعميد: O\n##ا: O\nلكل: B-ORG\n##ية: B-ORG\nالهندسة: L-ORG\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: اللغة الإسبانية منتشرة في قارة أمريكا الجنوبية.\n[CLS]: O\nاللغة: B-ANG\nالاسبانية: L-ANG\nمنتشرة: O\nفي: O\nقارة: O\nامريكا: B-LOC\nالجنوبية: L-LOC\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: زار الفريق الرئاسي قصر قرطاج الرئاسي في تونس.\n[CLS]: O\nزار: O\nالفريق: O\nالرياسي: O\nقصر: B-FAC\nقرطاج: L-FAC\nالرياسي: L-FAC\nفي: O\nتونس: B-GPE\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: يبدأ مهرجان كان السينمائي في مايو من كل عام.\n[CLS]: O\nيبدا: O\nمهرجان: B-EVE\nكان: L-EVE\nالسينمايي: O\nفي: O\nمايو: B-TIMEX\nمن: O\nكل: O\nعام: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: تعد جبال الهيمالايا من أعلى سلاسل الجبال في العالم.\n[CLS]: O\nتعد: O\nجبال: B-LOC\nالهي: L-LOC\n##مال: L-LOC\n##ايا: L-LOC\nمن: O\nاعلى: O\nسلاسل: O\nالجبال: O\nفي: O\nالعالم: O\n.: O\n[SEP]: O\n--------------------------------------------------\nSentence: تم إعلان الفائز بجائزة نوبل للسلام هذا الأسبوع.\n[CLS]: O\nتم: O\nاعلان: O\nالفايز: B-PER\nبجايزة: O\nنوبل: B-MISC\nللسلام: L-MISC\nهذا: O\nالاسبوع: O\n.: O\n[SEP]: O\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import os\n\n# Step 1: Save the model and tokenizer\nsave_directory = \"/kaggle/working/FinalModel\"\nos.makedirs(save_directory, exist_ok=True)\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\nprint(f\"✅ Model and tokenizer have been saved to {save_directory}\")\n\n# Step 2: Zip the directory\n!zip -r /kaggle/working/FinalModel.zip /kaggle/working/FinalModel\nprint(\"✅ Model has been zipped successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:04:21.526810Z","iopub.execute_input":"2025-05-12T21:04:21.527042Z","iopub.status.idle":"2025-05-12T21:04:45.081050Z","shell.execute_reply.started":"2025-05-12T21:04:21.527022Z","shell.execute_reply":"2025-05-12T21:04:45.080321Z"}},"outputs":[{"name":"stdout","text":"✅ Model and tokenizer have been saved to /kaggle/working/FinalModel\n  adding: kaggle/working/FinalModel/ (stored 0%)\n  adding: kaggle/working/FinalModel/model.safetensors (deflated 7%)\n  adding: kaggle/working/FinalModel/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/FinalModel/config.json (deflated 63%)\n  adding: kaggle/working/FinalModel/vocab.txt (deflated 63%)\n  adding: kaggle/working/FinalModel/tokenizer.json (deflated 73%)\n  adding: kaggle/working/FinalModel/tokenizer_config.json (deflated 74%)\n✅ Model has been zipped successfully!\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# رابط لتحميل الملف مباشرة\nFileLink('/kaggle/working/FinalModel.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:04:45.082012Z","iopub.execute_input":"2025-05-12T21:04:45.082314Z","iopub.status.idle":"2025-05-12T21:04:45.087858Z","shell.execute_reply.started":"2025-05-12T21:04:45.082286Z","shell.execute_reply":"2025-05-12T21:04:45.087100Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/FinalModel.zip","text/html":"<a href='/kaggle/working/FinalModel.zip' target='_blank'>/kaggle/working/FinalModel.zip</a><br>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}